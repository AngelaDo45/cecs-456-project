{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "from skimage import io, transform\n",
    "from PIL import Image as im\n",
    "import cv2\n",
    "\n",
    "\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "# from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not using this! \n",
    "def load_images(folder):\n",
    "    images = [] \n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "#         print(filename)\n",
    "        image = io.imread(os.path.join(folder, filename))\n",
    "        if image is not None:\n",
    "            if filename[-5] == 'L':\n",
    "                images.append(image)\n",
    "#                 images.append(image)\n",
    "                labels.append(filename[-6])\n",
    "            # display_image(image)\n",
    "    images =  np.asarray(images)\n",
    "    labels = np.asarray(labels)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the images using cv2. This will give us the right dimenstion (5, 32, 32, 3)\n",
    "def load(folder):\n",
    "    images = [] \n",
    "    labels = [] \n",
    "    for filename in os.listdir(folder):\n",
    "        image = cv2.imread(os.path.join(folder, filename))\n",
    "        if image is not None:\n",
    "            if filename[-5] == 'L':\n",
    "                image = cv2.resize(image, (32,32))\n",
    "                images.append(image)\n",
    "                labels.append(filename[-6])\n",
    "    images =  np.asarray(images)\n",
    "    labels = np.asarray(labels)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(img):\n",
    "    plt.figure()\n",
    "    plt.imshow(img) \n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 128, 128)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train, X_test = load_images('./try/')\n",
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X training dataset:  (9000, 32, 32, 3)\n",
      "Y training dataset:  (9000,)\n",
      "X testing dataset:  (1800, 32, 32, 3)\n",
      "Y testing dataset:  (1800,)\n"
     ]
    }
   ],
   "source": [
    "# obtain the training and testing dataset from the data folder\n",
    "X_train, Y_train = load('./data/train/')\n",
    "X_test, Y_test = load('./data/test/')\n",
    "\n",
    "print(\"X training dataset: \", X_train.shape)\n",
    "print(\"Y training dataset: \", Y_train.shape)\n",
    "print(\"X testing dataset: \", X_test.shape)\n",
    "print(\"Y testing dataset: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[47, 47, 47],\n",
       "        [43, 43, 43],\n",
       "        [41, 41, 41],\n",
       "        ...,\n",
       "        [45, 45, 45],\n",
       "        [50, 50, 50],\n",
       "        [50, 50, 50]],\n",
       "\n",
       "       [[46, 46, 46],\n",
       "        [45, 45, 45],\n",
       "        [41, 41, 41],\n",
       "        ...,\n",
       "        [52, 52, 52],\n",
       "        [51, 51, 51],\n",
       "        [54, 54, 54]],\n",
       "\n",
       "       [[42, 42, 42],\n",
       "        [36, 36, 36],\n",
       "        [38, 38, 38],\n",
       "        ...,\n",
       "        [45, 45, 45],\n",
       "        [48, 48, 48],\n",
       "        [45, 45, 45]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[43, 43, 43],\n",
       "        [44, 44, 44],\n",
       "        [43, 43, 43],\n",
       "        ...,\n",
       "        [53, 53, 53],\n",
       "        [49, 49, 49],\n",
       "        [51, 51, 51]],\n",
       "\n",
       "       [[48, 48, 48],\n",
       "        [51, 51, 51],\n",
       "        [43, 43, 43],\n",
       "        ...,\n",
       "        [56, 56, 56],\n",
       "        [49, 49, 49],\n",
       "        [50, 50, 50]],\n",
       "\n",
       "       [[52, 52, 52],\n",
       "        [45, 45, 45],\n",
       "        [45, 45, 45],\n",
       "        ...,\n",
       "        [52, 52, 52],\n",
       "        [57, 57, 57],\n",
       "        [50, 50, 50]]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[444].shape\n",
    "X_train[444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(Y_train[444])\n",
    "plt.imshow(X_train[444]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 6\n",
    "\n",
    "Y_train = np_utils.to_categorical(Y_train, num_classes)\n",
    "Y_test = np_utils.to_categorical(Y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.18039216, 0.18039216, 0.18039216],\n",
       "        [0.16470589, 0.16470589, 0.16470589],\n",
       "        [0.14509805, 0.14509805, 0.14509805],\n",
       "        ...,\n",
       "        [0.18431373, 0.18431373, 0.18431373],\n",
       "        [0.16470589, 0.16470589, 0.16470589],\n",
       "        [0.16078432, 0.16078432, 0.16078432]],\n",
       "\n",
       "       [[0.18039216, 0.18039216, 0.18039216],\n",
       "        [0.16078432, 0.16078432, 0.16078432],\n",
       "        [0.14901961, 0.14901961, 0.14901961],\n",
       "        ...,\n",
       "        [0.16862746, 0.16862746, 0.16862746],\n",
       "        [0.15294118, 0.15294118, 0.15294118],\n",
       "        [0.14509805, 0.14509805, 0.14509805]],\n",
       "\n",
       "       [[0.12941177, 0.12941177, 0.12941177],\n",
       "        [0.15294118, 0.15294118, 0.15294118],\n",
       "        [0.12941177, 0.12941177, 0.12941177],\n",
       "        ...,\n",
       "        [0.18431373, 0.18431373, 0.18431373],\n",
       "        [0.16862746, 0.16862746, 0.16862746],\n",
       "        [0.15294118, 0.15294118, 0.15294118]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.16862746, 0.16862746, 0.16862746],\n",
       "        [0.14901961, 0.14901961, 0.14901961],\n",
       "        [0.16078432, 0.16078432, 0.16078432],\n",
       "        ...,\n",
       "        [0.12941177, 0.12941177, 0.12941177],\n",
       "        [0.12941177, 0.12941177, 0.12941177],\n",
       "        [0.12156863, 0.12156863, 0.12156863]],\n",
       "\n",
       "       [[0.15686275, 0.15686275, 0.15686275],\n",
       "        [0.15294118, 0.15294118, 0.15294118],\n",
       "        [0.14901961, 0.14901961, 0.14901961],\n",
       "        ...,\n",
       "        [0.14901961, 0.14901961, 0.14901961],\n",
       "        [0.14117648, 0.14117648, 0.14117648],\n",
       "        [0.13725491, 0.13725491, 0.13725491]],\n",
       "\n",
       "       [[0.16470589, 0.16470589, 0.16470589],\n",
       "        [0.16078432, 0.16078432, 0.16078432],\n",
       "        [0.16078432, 0.16078432, 0.16078432],\n",
       "        ...,\n",
       "        [0.17254902, 0.17254902, 0.17254902],\n",
       "        [0.15294118, 0.15294118, 0.15294118],\n",
       "        [0.15686275, 0.15686275, 0.15686275]]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[444]\n",
    "X_test[444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 16, 16, 16)        448       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 32)          4640      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 198       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 14,534\n",
      "Trainable params: 14,534\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "\n",
    "model_1.add(Conv2D(16, (3, 3), strides = (2,2), padding='same',\n",
    "                 input_shape=X_train.shape[1:]))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "model_1.add(Conv2D(32, (3, 3), strides = (2,2)))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1.add(Dropout(0.25))\n",
    "\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(32))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(num_classes))\n",
    "model_1.add(Activation('softmax'))\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 1.5794 - accuracy: 0.3711 - val_loss: 0.4156 - val_accuracy: 0.9561\n",
      "Epoch 2/20\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 0.5788 - accuracy: 0.8163 - val_loss: 0.1148 - val_accuracy: 0.9706\n",
      "Epoch 3/20\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 0.2750 - accuracy: 0.9150 - val_loss: 0.0632 - val_accuracy: 0.9789\n",
      "Epoch 4/20\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 0.1968 - accuracy: 0.9391 - val_loss: 0.0468 - val_accuracy: 0.9856\n",
      "Epoch 5/20\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 0.1393 - accuracy: 0.9535 - val_loss: 0.0391 - val_accuracy: 0.9878\n",
      "Epoch 6/20\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9576 - val_loss: 0.0229 - val_accuracy: 0.9922\n",
      "Epoch 7/20\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 0.1082 - accuracy: 0.9636 - val_loss: 0.0174 - val_accuracy: 0.9939\n",
      "Epoch 8/20\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 0.0889 - accuracy: 0.9671 - val_loss: 0.0179 - val_accuracy: 0.9944\n",
      "Epoch 9/20\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 0.0744 - accuracy: 0.9746 - val_loss: 0.0101 - val_accuracy: 0.9961\n",
      "Epoch 10/20\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 0.0595 - accuracy: 0.9798 - val_loss: 0.0099 - val_accuracy: 0.9961\n",
      "Epoch 11/20\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 0.0607 - accuracy: 0.9787 - val_loss: 0.0120 - val_accuracy: 0.9961\n",
      "Epoch 12/20\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 0.0631 - accuracy: 0.9778 - val_loss: 0.0103 - val_accuracy: 0.9961\n",
      "Epoch 13/20\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 0.0512 - accuracy: 0.9811 - val_loss: 0.0043 - val_accuracy: 0.9978\n",
      "Epoch 14/20\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 0.0405 - accuracy: 0.9853 - val_loss: 0.0056 - val_accuracy: 0.9972\n",
      "Epoch 15/20\n",
      "282/282 [==============================] - 1s 3ms/step - loss: 0.0439 - accuracy: 0.9843 - val_loss: 0.0044 - val_accuracy: 0.9983\n",
      "Epoch 16/20\n",
      "282/282 [==============================] - 1s 3ms/step - loss: 0.0380 - accuracy: 0.9850 - val_loss: 0.0035 - val_accuracy: 0.9983\n",
      "Epoch 17/20\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 0.0357 - accuracy: 0.9872 - val_loss: 0.0038 - val_accuracy: 0.9983\n",
      "Epoch 18/20\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 0.0388 - accuracy: 0.9863 - val_loss: 0.0028 - val_accuracy: 0.9989\n",
      "Epoch 19/20\n",
      "282/282 [==============================] - 1s 2ms/step - loss: 0.0353 - accuracy: 0.9889 - val_loss: 0.0030 - val_accuracy: 0.9983\n",
      "Epoch 20/20\n",
      "282/282 [==============================] - 1s 3ms/step - loss: 0.0372 - accuracy: 0.9869 - val_loss: 0.0012 - val_accuracy: 0.9994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22475013af0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.0005, decay=1e-6)\n",
    "\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_1.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=10,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "References \n",
    " - https://github.com/evernext10/Hand-Gesture-Recognition-Machine-Learning\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
